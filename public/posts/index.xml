<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on fazi</title>
		<link>/posts/</link>
		<description>Recent content in Posts on fazi</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
		<atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Classifying Divorce</title>
			<link>/posts/divorce/</link>
			<pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
			
			<guid>/posts/divorce/</guid>
			<description>IntroductionThe following analysis is a classification problem, where the goal is to try and predict divorce based on a “Divorce Predictors Scale”. The data was developed based on a psychology study named Gottman couples therapy, and was freely obtained via http://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set. The link to the original study from which this data emerged is https://dergipark.org.tr/en/pub/nevsosbilen/issue/46568/549416. The aim here is to utilise the answers by couples to 54 questions regarding their marriage to try and predict whether the marriage is intact or if divorce is imminent.</description>
			<content type="html"><![CDATA[


<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>The following analysis is a classification problem, where the goal is to try and predict divorce based on a “Divorce Predictors Scale”. The data was developed based on a psychology study named Gottman couples therapy, and was freely obtained via <a href="http://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set" class="uri">http://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set</a>. The link to the original study from which this data emerged is <a href="https://dergipark.org.tr/en/pub/nevsosbilen/issue/46568/549416" class="uri">https://dergipark.org.tr/en/pub/nevsosbilen/issue/46568/549416</a>. The aim here is to utilise the answers by couples to 54 questions regarding their marriage to try and predict whether the marriage is intact or if divorce is imminent.</p>
<p>Loading the data and required libraries:</p>
<pre class="r"><code>#Libraries...
library(corrplot)
library(caret)
library(tidyverse)
library(rsample)
library(ROCR)
library(vip)

#Loading data...
divorce &lt;- read.csv(&quot;divorce.csv&quot;, sep = &quot;;&quot;)</code></pre>
</div>
<div id="visualizing-and-exploring" class="section level3">
<h3>Visualizing and Exploring</h3>
<p>Before progressing forward, it may be helpful to list all the questions as a quick reference if required. The 54 questions are as follows:</p>
<p>Status (dependent variable) - describes whether divorced (1) or married (0)</p>
<ol style="list-style-type: decimal">
<li>If one of us apologizes when our discussion deteriorates, the discussion ends.</li>
<li>I know we can ignore our differences, even if things get hard sometimes.</li>
<li>When we need it, we can take our discussions with my spouse from the beginning and correct it.</li>
<li>When I discuss with my spouse, to contact him will eventually work.</li>
<li>The time I spent with my wife is special for us.</li>
<li>We don’t have time at home as partners.</li>
<li>We are like two strangers who share the same environment at home rather than family.</li>
<li>I enjoy our holidays with my wife.</li>
<li>I enjoy traveling with my wife.</li>
<li>Most of our goals are common to my spouse.</li>
<li>I think that one day in the future, when I look back, I see that my spouse and I have been in harmony with each other.</li>
<li>My spouse and I have similar values in terms of personal freedom.</li>
<li>My spouse and I have similar sense of entertainment.</li>
<li>Most of our goals for people (children, friends, etc.) are the same.</li>
<li>Our dreams with my spouse are similar and harmonious.</li>
<li>We’re compatible with my spouse about what love should be.</li>
<li>We share the same views about being happy in our life with my spouse</li>
<li>My spouse and I have similar ideas about how marriage should be</li>
<li>My spouse and I have similar ideas about how roles should be in marriage</li>
<li>My spouse and I have similar values in trust.</li>
<li>I know exactly what my wife likes.</li>
<li>I know how my spouse wants to be taken care of when she/he sick.</li>
<li>I know my spouse’s favorite food.</li>
<li>I can tell you what kind of stress my spouse is facing in her/his life.</li>
<li>I have knowledge of my spouse’s inner world.</li>
<li>I know my spouse’s basic anxieties.</li>
<li>I know what my spouse’s current sources of stress are.</li>
<li>I know my spouse’s hopes and wishes.</li>
<li>I know my spouse very well.</li>
<li>I know my spouse’s friends and their social relationships.</li>
<li>I feel aggressive when I argue with my spouse.</li>
<li>When discussing with my spouse, I usually use expressions such as ‘you always’ or ‘you never’ .</li>
<li>I can use negative statements about my spouse’s personality during our discussions.</li>
<li>I can use offensive expressions during our discussions.</li>
<li>I can insult my spouse during our discussions.</li>
<li>I can be humiliating when we discussions.</li>
<li>My discussion with my spouse is not calm.</li>
<li>I hate my spouse’s way of open a subject.</li>
<li>Our discussions often occur suddenly.</li>
<li>We’re just starting a discussion before I know what’s going on.</li>
<li>When I talk to my spouse about something, my calm suddenly breaks.</li>
<li>When I argue with my spouse, ı only go out and I don’t say a word.</li>
<li>I mostly stay silent to calm the environment a little bit.</li>
<li>Sometimes I think it’s good for me to leave home for a while.</li>
<li>I’d rather stay silent than discuss with my spouse.</li>
<li>Even if I’m right in the discussion, I stay silent to hurt my spouse.</li>
<li>When I discuss with my spouse, I stay silent because I am afraid of not being able to control my anger.</li>
<li>I feel right in our discussions.</li>
<li>I have nothing to do with what I’ve been accused of.</li>
<li>I’m not actually the one who’s guilty about what I’m accused of.</li>
<li>I’m not the one who’s wrong about problems at home.</li>
<li>I wouldn’t hesitate to tell my spouse about her/his inadequacy.</li>
<li>When I discuss, I remind my spouse of her/his inadequacy.</li>
<li>I’m not afraid to tell my spouse about her/his incompetence.</li>
</ol>
<p>As all the answers are categorical by nature (a single value between 0 and 4), the variables need to be converted to a factor first before moving forward with the analysis.</p>
<pre class="r"><code>divorce[,1:55] &lt;- lapply(divorce[,1:55], factor) # convert all to factor
colnames(divorce)[55] &lt;- &quot;Status&quot; #change dependent variable name

summary(divorce$Status) #a summary of the dependent variable</code></pre>
<pre><code>##  0  1 
## 86 84</code></pre>
<p>From the summary values above, it is clear that the two classes (1 and 0) are almost equal in quantity, therefore there is no class imbalance problem. This luckily makes the analysis simpler to perform.</p>
<p>To further understand the independent variables, a correlation plot is developed to see if any of the questions provide approximately the same information to the data.</p>
<pre class="r"><code>divorce_num &lt;- divorce[,1:54] #temporary dataframe created
divorce_num[,1:54] &lt;- sapply(divorce_num[,1:54], as.numeric) #reconverting back to numeric 
#for the correlation function to work

div_cor &lt;- cor(divorce_num)
corrplot(div_cor, method = &quot;shade&quot;)</code></pre>
<p><img src="/posts/Divorce_files/figure-html/unnamed-chunk-3-1.png" width="1152" /></p>
<p>The correlation plot does provide some interesting insights. Questions 6 and 7 have very little to no correlation with any of the other questions. There are also certain chunks of questions that apparently have a higher correlation with each other than the general average, such as from Question 8 to 30. However, the overall high correlations are not that surprising, as the questions all do have a central theme of marriage and relationships in common, so in some way it is plausible to assume they will all be interlinked with each other in some way. Despite this, some extremely high correlated questions can be removed to get rid of any redundant features.</p>
<pre class="r"><code>findCorrelation(div_cor,
                cutoff = 0.95,
                names = TRUE) #finding questions that have a correlation of 0.95 or higher. </code></pre>
<pre><code>## [1] &quot;Atr19&quot; &quot;Atr36&quot; &quot;Atr20&quot;</code></pre>
<pre class="r"><code>divorce &lt;- divorce[,c(-19, -36)]</code></pre>
<p>Questions 19, 36 and 20 are highly correlated with each other, so 19 and 36 are removed, leaving 20 (a personal choice). To conclude visualizing, the class distribution of the dependent variable can also be plotted.</p>
<pre class="r"><code>#Visualizing class distribution of dependent variable...
x &lt;- as.data.frame(table(divorce[,53]))
names(x)[1] &lt;- &quot;Status&quot;
ggplot(x, aes(Status, Freq, label = Freq, fill = Status)) +
  geom_col() +
  geom_text(nudge_y = 1.5) +
  scale_fill_discrete(name = &quot;Marital Status&quot;,
                      breaks = c(0, 1),
                      labels = c(&quot;Married&quot;,&quot;Divorced&quot;))</code></pre>
<p><img src="/posts/Divorce_files/figure-html/unnamed-chunk-5-1.png" width="864" /></p>
<pre class="r"><code>#Removing temporary dataframes
rm(divorce_num)
rm(x)</code></pre>
<p>As previously mentioned, the number of married and divorced couples answering the questions is almost equally balanced.</p>
</div>
<div id="modelling" class="section level3">
<h3>Modelling</h3>
<p>As this is a simple binary classifaction problem (only two outcomes), the interpretability of the process is not too complicated. The data is partitioned using a 70/30 train-test split, and a 10-fold repeated cross validation resampling strategy is applied. The models used for classification predictions are Naive Bayes (NB), Logistic Regression (Logi), Generalized Linear Model with Regularization Parameters (RGZ) and Support Vector Machines (SVM). To sccuinctly and briefly describe the methods: NB uses probabilistic theory based on Bayes Theorum to classify variables, Logi is a log transformed version of the standard linear regression model making it suitable for classification as opposed to regression, RGZ is version of generalized linear models involving penalty parameters aimed to improve performance when there are numerous feaures involved, and SVM is an advanced technique where features classified on a simulated feature by a specific function utilising linear algebra techniques.</p>
<p>The goal is to find the best model among these four that produces the most accurate predictions. Along with the standard accuracy metric used in the confusion matrices in classification problem, the Receiver Operating Characteristics (ROC) curve will also be utilised, since it provides a more robust validation of the accuracy. It should be noted however, that in cases of binary classifactions, the area under the curve (AUC) for the ROC and the accuracy values are the same. The data is split and the models are run in the following code block.</p>
<pre class="r"><code>#Train-Test Split...
set.seed(1234)
split &lt;- initial_split(divorce, prop = 0.7, strata = &quot;Status&quot;)
train &lt;- training(split)
test &lt;- testing(split)

cv &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10)

#Naive Bayes... (benchmark)
set.seed(1234)
divorce_nbayes &lt;- train(Status ~ .,
                        data = train,
                        method = &quot;nb&quot;,
                        trControl = cv)
#Logistic..
set.seed(1234)
divorce_logi &lt;- train(Status ~ .,
                      data = train,
                      method = &quot;glm&quot;,
                      family = &quot;binomial&quot;,
                      trControl = cv)
#GLM Regularized..
set.seed(1234)
divorce_rgz &lt;- train(Status ~ .,
                     data = train,
                     method = &quot;glmnet&quot;,
                     trControl = cv)
#SVM...
set.seed(1234)
divorce_svm &lt;- train(Status ~ .,
                     data = train,
                     method = &quot;svmRadial&quot;,
                     trControl = cv)</code></pre>
<p>To compare the accuracy of the models on the training set, both the accuracy and the Kappa values are compared. The Kappa value is thought to be more robust as it takes into account the success of the predictions based solely on chance.</p>
<pre class="r"><code>summary(resamples(list(
  N_Bayes = divorce_nbayes,
  Logistic = divorce_logi,
  Regularized = divorce_rgz,
  SVM  = divorce_svm
)))$statistics$Accuracy</code></pre>
<pre><code>##                  Min.   1st Qu. Median      Mean   3rd Qu. Max. NA&#39;s
## N_Bayes     0.4545455 0.6666667   0.75 0.7454196 0.8333333    1    0
## Logistic    0.6666667 0.9166667   1.00 0.9442599 1.0000000    1    0
## Regularized 0.8333333 0.9166667   1.00 0.9659499 1.0000000    1    0
## SVM         0.8333333 0.9166667   1.00 0.9751282 1.0000000    1    0</code></pre>
<pre class="r"><code>summary(resamples(list(
  N_Bayes = divorce_nbayes,
  Logistic = divorce_logi,
  Regularized = divorce_rgz,
  SVM  = divorce_svm
)))$statistics$Kappa</code></pre>
<pre><code>##                  Min.   1st Qu. Median      Mean   3rd Qu. Max. NA&#39;s
## N_Bayes     0.0000000 0.3333333    0.5 0.4976498 0.6666667    1    0
## Logistic    0.3333333 0.8333333    1.0 0.8883405 1.0000000    1    0
## Regularized 0.6666667 0.8333333    1.0 0.9317701 1.0000000    1    0
## SVM         0.6666667 0.8333333    1.0 0.9502008 1.0000000    1    0</code></pre>
<p>The top table highlights the accuracy values, while the bottom table highlights the Kappa values. In both cases, the Naive Bayes (benchmark model) performed the worst, with SVM performing the best. However, the RGZ Model is also very close, which warrants further inspection. It is also highly possible that the SVM due to its complexity may be overfitting. The next step is to analyse the confusion matrices for each of the models. The “positive” class in this case is 1 (divorce).</p>
<pre class="r"><code>nbayes_pred &lt;- predict(divorce_nbayes, test) #Naive Bayes 
confusionMatrix(data = relevel(nbayes_pred, ref = 2),
                reference = relevel(test$Status, ref = 2)) </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  1  0
##          1 24 15
##          0  1 10
##                                          
##                Accuracy : 0.68           
##                  95% CI : (0.533, 0.8048)
##     No Information Rate : 0.5            
##     P-Value [Acc &gt; NIR] : 0.007673       
##                                          
##                   Kappa : 0.36           
##                                          
##  Mcnemar&#39;s Test P-Value : 0.001154       
##                                          
##             Sensitivity : 0.9600         
##             Specificity : 0.4000         
##          Pos Pred Value : 0.6154         
##          Neg Pred Value : 0.9091         
##              Prevalence : 0.5000         
##          Detection Rate : 0.4800         
##    Detection Prevalence : 0.7800         
##       Balanced Accuracy : 0.6800         
##                                          
##        &#39;Positive&#39; Class : 1              
## </code></pre>
<pre class="r"><code>logi_pred &lt;- predict(divorce_logi, test) #Logistic
confusionMatrix(data = relevel(logi_pred, ref = 2),
                reference = relevel(test$Status, ref = 2)) </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  1  0
##          1 21  0
##          0  4 25
##                                           
##                Accuracy : 0.92            
##                  95% CI : (0.8077, 0.9778)
##     No Information Rate : 0.5             
##     P-Value [Acc &gt; NIR] : 2.231e-10       
##                                           
##                   Kappa : 0.84            
##                                           
##  Mcnemar&#39;s Test P-Value : 0.1336          
##                                           
##             Sensitivity : 0.8400          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.8621          
##              Prevalence : 0.5000          
##          Detection Rate : 0.4200          
##    Detection Prevalence : 0.4200          
##       Balanced Accuracy : 0.9200          
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
<pre class="r"><code>rgz_pred &lt;- predict(divorce_rgz, test) #Regularized
confusionMatrix(data = relevel(rgz_pred, ref = 2),
                reference = relevel(test$Status, ref = 2)) </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  1  0
##          1 24  0
##          0  1 25
##                                           
##                Accuracy : 0.98            
##                  95% CI : (0.8935, 0.9995)
##     No Information Rate : 0.5             
##     P-Value [Acc &gt; NIR] : 4.53e-14        
##                                           
##                   Kappa : 0.96            
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9600          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9615          
##              Prevalence : 0.5000          
##          Detection Rate : 0.4800          
##    Detection Prevalence : 0.4800          
##       Balanced Accuracy : 0.9800          
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
<pre class="r"><code>svm_pred &lt;- predict(divorce_svm, test) #SVM
confusionMatrix(data = relevel(svm_pred, ref = 2),
                reference = relevel(test$Status, ref = 2)) </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  1  0
##          1 24  0
##          0  1 25
##                                           
##                Accuracy : 0.98            
##                  95% CI : (0.8935, 0.9995)
##     No Information Rate : 0.5             
##     P-Value [Acc &gt; NIR] : 4.53e-14        
##                                           
##                   Kappa : 0.96            
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9600          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9615          
##              Prevalence : 0.5000          
##          Detection Rate : 0.4800          
##    Detection Prevalence : 0.4800          
##       Balanced Accuracy : 0.9800          
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
<p>Once again, the Naive Bayes model produces the worst results. However, this time the RGZ model and the SVM model have performed exactly the same, with equal accuracy, Kappa and confusion matrix values. This allows for the possibility of actually choosing the RGZ model instead of the SVM, as they are both performing equally, and the RGZ model is preferable as it is much easier to interpret. However, just to be sure, the AUC values for the models can be computed as well.</p>
<pre class="r"><code>pred_ROCR_nbayes &lt;- prediction(as.numeric(nbayes_pred), 
                               as.numeric(test$Status))

perf_ROCR_nbayes &lt;- performance(pred_ROCR_nbayes, 
                                measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)

auc_ROCR_nbayes &lt;- performance(pred_ROCR_nbayes, measure = &quot;auc&quot;)
auc_ROCR_nbayes &lt;- auc_ROCR_nbayes@y.values[[1]]

pred_ROCR_logi &lt;- prediction(as.numeric(logi_pred), 
                             as.numeric(test$Status))

perf_ROCR_logi &lt;- performance(pred_ROCR_logi, 
                              measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)

auc_ROCR_logi &lt;- performance(pred_ROCR_logi, measure = &quot;auc&quot;)
auc_ROCR_logi &lt;- auc_ROCR_logi@y.values[[1]]

pred_ROCR_rgz &lt;- prediction(as.numeric(rgz_pred), 
                             as.numeric(test$Status))

perf_ROCR_rgz &lt;- performance(pred_ROCR_rgz, 
                              measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)

auc_ROCR_rgz &lt;- performance(pred_ROCR_rgz, measure = &quot;auc&quot;)
auc_ROCR_rgz &lt;- auc_ROCR_rgz@y.values[[1]]

pred_ROCR_svm &lt;- prediction(as.numeric(svm_pred), 
                            as.numeric(test$Status))

perf_ROCR_svm &lt;- performance(pred_ROCR_svm, 
                             measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)

auc_ROCR_svm &lt;- performance(pred_ROCR_svm, measure = &quot;auc&quot;)
auc_ROCR_svm &lt;- auc_ROCR_svm@y.values[[1]]

data.frame(&quot;Naive Bayes&quot; = auc_ROCR_nbayes,&quot;Logistic&quot; = auc_ROCR_logi, &quot;RGZ&quot; = auc_ROCR_rgz, &quot;SVM&quot; = auc_ROCR_svm) </code></pre>
<pre><code>##   Naive.Bayes Logistic  RGZ  SVM
## 1        0.68     0.92 0.98 0.98</code></pre>
<p>As expected, the accuracy and the AUC values for the RGZ and SVM models are the same. As both the RGZ and SVM are performing equally well, the RGZ model is preferred to be the final model. Just for a visual, the ROC curves for Naive Bayes, Logistic and RGZ models are plotted below. The curve for RGZ resembles an almost perfect ROC curve, which is backed up by the exceptionally high accuracy.</p>
<pre class="r"><code>plot(perf_ROCR_nbayes, col = &quot;black&quot;, lty = 1)
plot(perf_ROCR_logi, add = TRUE, col = &quot;red&quot;, lty = 2)
plot(perf_ROCR_rgz, add = TRUE, col = &quot;blue&quot;, lty = 3)
legend(0.8, 0.2, legend = c(&quot;Naive Bayes&quot;, &quot;Logistic&quot;, &quot;RGZ&quot;),
       col = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lty = c(1, 2, 3), cex = 1)</code></pre>
<p><img src="/posts/Divorce_files/figure-html/unnamed-chunk-10-1.png" width="864" /></p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>Since the Generalized Linear Models with Regularization Parameters is being used, the actual parameter values should be noted as well.</p>
<pre class="r"><code>divorce_rgz</code></pre>
<pre><code>## glmnet 
## 
## 120 samples
##  52 predictor
##   2 classes: &#39;0&#39;, &#39;1&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 108, 108, 107, 109, 108, 108, ... 
## Resampling results across tuning parameters:
## 
##   alpha  lambda      Accuracy   Kappa    
##   0.10   0.02517439  0.9659499  0.9317701
##   0.10   0.07960842  0.9626049  0.9249679
##   0.10   0.25174392  0.9601049  0.9199679
##   0.55   0.02517439  0.9474534  0.8944101
##   0.55   0.07960842  0.9424534  0.8845724
##   0.55   0.25174392  0.9150466  0.8291400
##   1.00   0.02517439  0.9373776  0.8744358
##   1.00   0.07960842  0.9348776  0.8693976
##   1.00   0.25174392  0.8833566  0.7654431
## 
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were alpha = 0.1 and lambda = 0.02517439.</code></pre>
<p>The model maximized accuracy with an alpha value of 0.1 and lambda value of 0.025. Having found the exact model parameters, an easy to interpret model has been developed for this specific use case which provides exceptionally accurate results.</p>
</div>
]]></content>
		</item>
		
		<item>
			<title>Drugs and Memories - The impact of Interaction in Regressions</title>
			<link>/posts/drugs-and-memories/</link>
			<pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
			
			<guid>/posts/drugs-and-memories/</guid>
			<description>IntroductionThis is a simple analysis examining how variables interacting with each other can impact regression performance. The data set has only nine features, out of which there will be further feature selection, thereby making it easy to understand the effects. The data was freely obtained from https://www.kaggle.com/steveahn/memory-test-on-drugged-islanders-data. It describes the results of an experiment which observes the effects of anti-anxiety medicine on memory recall, while simultaneously including the effect of happy and sad memories.</description>
			<content type="html"><![CDATA[
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>This is a simple analysis examining how variables interacting with each other can impact regression performance. The data set has only nine features, out of which there will be further feature selection, thereby making it easy to understand the effects. The data was freely obtained from <a href="https://www.kaggle.com/steveahn/memory-test-on-drugged-islanders-data" class="uri">https://www.kaggle.com/steveahn/memory-test-on-drugged-islanders-data</a>. It describes the results of an experiment which observes the effects of anti-anxiety medicine on memory recall, while simultaneously including the effect of happy and sad memories. Three different drugs were used, along with three different levels of dosage. The results (in seconds) of how long it took to finish a memory test before drug exposure and after being exposed are noted in two separate columns, with the difference between these two noted separately. The difference is the target variable in this case. The age of the participants are also considered. The analysis is focused on being as simple as possible, as the primary goal is examining how results change when the interaction between variables is taken into account. More information regarding the data can be found in the provided link.</p>
<p>Loading the data and required libraries:</p>
<pre class="r"><code>#Libraries...
library(tidyverse)
library(reshape)
library(GGally)
library(rsample)
library(caret)
library(plotly)

#Data...
memory &lt;- read.csv(&quot;Islander-data.csv&quot;)

#A glimpse...
head(memory)</code></pre>
<pre><code>##   first_name last_name age Happy_Sad_group Dosage Drug Mem_Score_Before
## 1    Bastian  Carrasco  25               H      1    A             63.5
## 2       Evan  Carrasco  52               S      1    A             41.6
## 3  Florencia  Carrasco  29               H      1    A             59.7
## 4      Holly  Carrasco  50               S      1    A             51.7
## 5     Justin  Carrasco  52               H      1    A             47.0
## 6       Liam  Carrasco  37               S      1    A             66.4
##   Mem_Score_After Diff
## 1            61.2 -2.3
## 2            40.7 -0.9
## 3            55.1 -4.6
## 4            51.2 -0.5
## 5            47.1  0.1
## 6            58.1 -8.3</code></pre>
</div>
<div id="processing-and-exploring" class="section level3">
<h3>Processing and Exploring</h3>
<p>Before moving forward, the data needs to be processed slightly to simplify a few things. The columns containing first and last names are removed, as they are not necessarily required for this analysis. Also, Dosage needs to be converted to type factor as this is actually a categorical variable.</p>
<pre class="r"><code>memory &lt;- memory[, -1:-2] #removing columns with names
memory[, 3] &lt;- as.factor(memory[, 3]) #converting Dosage to factor</code></pre>
<p>Firstly, the distributions of the age variable can be visualised to see how spread out the different ages of the participants are. The age distribution is right skewed, with majority of the participants falling between the ages 25 and 40. It is noteworthy that there is only one person who is 24 years old.</p>
<pre class="r"><code>seqage &lt;- as.vector(memory[, 1]) #create temporary vector for the visual

#age distribution
ggplotly(ggplot(memory, aes(age)) +
  geom_histogram(bins = 60, fill = &quot;lightblue&quot;, color = &quot;black&quot;) +
  scale_x_continuous(breaks = seqage) +
  scale_y_continuous(breaks = seq(0,20, by = 1)) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))) #Interactive</code></pre>
<div id="htmlwidget-1" style="width:720px;height:432px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"data":[{"orientation":"v","width":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"base":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"x":[24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83],"y":[1,10,7,9,10,12,5,3,7,7,11,11,4,13,7,4,7,2,4,2,2,3,3,2,4,5,5,6,6,3,4,1,2,0,0,1,1,0,2,2,0,2,1,0,2,1,0,0,1,1,0,0,0,0,0,0,1,0,0,1],"text":["count:  1<br />age: 24","count: 10<br />age: 25","count:  7<br />age: 26","count:  9<br />age: 27","count: 10<br />age: 28","count: 12<br />age: 29","count:  5<br />age: 30","count:  3<br />age: 31","count:  7<br />age: 32","count:  7<br />age: 33","count: 11<br />age: 34","count: 11<br />age: 35","count:  4<br />age: 36","count: 13<br />age: 37","count:  7<br />age: 38","count:  4<br />age: 39","count:  7<br />age: 40","count:  2<br />age: 41","count:  4<br />age: 42","count:  2<br />age: 43","count:  2<br />age: 44","count:  3<br />age: 45","count:  3<br />age: 46","count:  2<br />age: 47","count:  4<br />age: 48","count:  5<br />age: 49","count:  5<br />age: 50","count:  6<br />age: 51","count:  6<br />age: 52","count:  3<br />age: 53","count:  4<br />age: 54","count:  1<br />age: 55","count:  2<br />age: 56","count:  0<br />age: 57","count:  0<br />age: 58","count:  1<br />age: 59","count:  1<br />age: 60","count:  0<br />age: 61","count:  2<br />age: 62","count:  2<br />age: 63","count:  0<br />age: 64","count:  2<br />age: 65","count:  1<br />age: 66","count:  0<br />age: 67","count:  2<br />age: 68","count:  1<br />age: 69","count:  0<br />age: 70","count:  0<br />age: 71","count:  1<br />age: 72","count:  1<br />age: 73","count:  0<br />age: 74","count:  0<br />age: 75","count:  0<br />age: 76","count:  0<br />age: 77","count:  0<br />age: 78","count:  0<br />age: 79","count:  1<br />age: 80","count:  0<br />age: 81","count:  0<br />age: 82","count:  1<br />age: 83"],"type":"bar","marker":{"autocolorscale":false,"color":"rgba(173,216,230,1)","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":28.1765601217656,"r":7.30593607305936,"b":42.130898021309,"l":37.2602739726027},"plot_bgcolor":"rgba(255,255,255,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[20.5,86.5],"tickmode":"array","ticktext":["25","52","29","50","52","37","35","38","29","36","63","27","39","26","26","48","51","26","44","53","55","31","50","62","25","40","35","38","28","36","53","29","39","35","68","38","50","25","29","35","26","51","56","35","28","54","51","36","35","52","47","43","30","32","53","68","27","49","35","47","34","34","27","39","40","34","37","26","29","56","28","51","52","41","37","45","42","52","72","29","26","33","40","28","40","33","27","30","33","28","46","37","27","35","27","33","30","50","40","48","34","32","42","37","29","28","59","51","37","31","38","35","37","34","34","49","35","25","29","29","27","44","25","66","36","33","37","45","28","38","43","45","27","37","27","28","30","62","54","29","34","26","65","37","48","32","29","25","37","29","37","31","40","46","46","60","49","83","49","25","24","54","25","42","42","25","65","48","35","69","38","37","32","49","30","25","50","38","33","28","32","80","63","34","51","34","28","32","73","34","34","33","39","52","41","54","40","32"],"tickvals":[25,52,29,50,52,37,35,38,29,36,63,27,39,26,26,48,51,26,44,53,55,31,50,62,25,40,35,38,28,36,53,29,39,35,68,38,50,25,29,35,26,51,56,35,28,54,51,36,35,52,47,43,30,32,53,68,27,49,35,47,34,34,27,39,40,34,37,26,29,56,28,51,52,41,37,45,42,52,72,29,26,33,40,28,40,33,27,30,33,28,46,37,27,35,27,33,30,50,40,48,34,32,42,37,29,28,59,51,37,31,38,35,37,34,34,49,35,25,29,29,27,44,25,66,36,33,37,45,28,38,43,45,27,37,27,28,30,62,54,29,34,26,65,37,48,32,29,25,37,29,37,31,40,46,46,60,49,83,49,25,24,54,25,42,42,25,65,48,35,69,38,37,32,49,30,25,50,38,33,28,32,80,63,34,51,34,28,32,73,34,34,33,39,52,41,54,40,32],"categoryorder":"array","categoryarray":["25","52","29","50","52","37","35","38","29","36","63","27","39","26","26","48","51","26","44","53","55","31","50","62","25","40","35","38","28","36","53","29","39","35","68","38","50","25","29","35","26","51","56","35","28","54","51","36","35","52","47","43","30","32","53","68","27","49","35","47","34","34","27","39","40","34","37","26","29","56","28","51","52","41","37","45","42","52","72","29","26","33","40","28","40","33","27","30","33","28","46","37","27","35","27","33","30","50","40","48","34","32","42","37","29","28","59","51","37","31","38","35","37","34","34","49","35","25","29","29","27","44","25","66","36","33","37","45","28","38","43","45","27","37","27","28","30","62","54","29","34","26","65","37","48","32","29","25","37","29","37","31","40","46","46","60","49","83","49","25","24","54","25","42","42","25","65","48","35","69","38","37","32","49","30","25","50","38","33","28","32","80","63","34","51","34","28","32","73","34","34","33","39","52","41","54","40","32"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-45,"showline":true,"linecolor":"rgba(0,0,0,1)","linewidth":0.66417600664176,"showgrid":false,"gridcolor":null,"gridwidth":0,"zeroline":false,"anchor":"y","title":{"text":"age","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.65,13.65],"tickmode":"array","ticktext":["0","1","2","3","4","5","6","7","8","9","10","11","12","13"],"tickvals":[0,1,2,3,4,5,6,7,8,9,10,11,12,13],"categoryorder":"array","categoryarray":["0","1","2","3","4","5","6","7","8","9","10","11","12","13"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":true,"linecolor":"rgba(0,0,0,1)","linewidth":0.66417600664176,"showgrid":false,"gridcolor":null,"gridwidth":0,"zeroline":false,"anchor":"x","title":{"text":"count","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"415c5e7a4e62":{"x":{},"type":"bar"}},"cur_data":"415c5e7a4e62","visdat":{"415c5e7a4e62":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<pre class="r"><code>rm(seqage) #removing the temporary vector</code></pre>
<p>The distributions of the memory score variables can also be examined. It is interesting to see that the boxplots for the before and after memory scores are almost exactly the same. This indicates that the differences are only in the margin of a few seconds, which can be visualised by the distribution of the Diff variable.</p>
<pre class="r"><code>mem_melt &lt;- melt(memory[, 5:7]) #temporary dataframe for the visual

#memory scores distributions
ggplotly(ggplot(mem_melt, aes(variable, value, fill = variable)) +
  geom_boxplot() +
  theme_minimal())</code></pre>
<div id="htmlwidget-2" style="width:720px;height:432px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"data":[{"x":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"y":[63.5,41.6,59.7,51.7,47,66.4,44.1,76.3,56.2,54.8,90,52.3,35.5,85.6,42.3,53.5,48.3,64,74.3,45,52.1,79.9,55.7,46.5,48.5,47,75,90,43.9,74.9,74.5,58.9,36.4,58.8,59.9,40.2,74.2,50,84.4,40.8,87,46.5,64.4,90,60.9,46.4,55.2,61.8,65,28.3,41.9,49.4,43.6,71.7,54.8,41.9,81,46.7,31.7,65.6,57.3,72.6,54,61.6,59.8,64.1,53.3,75,90,49.2,54.5,49.3,66.2,56.2,46.9,45.8,41,65.1,76.2,39.6,48.3,42.5,56.9,74,63.3,53,44.1,59.6,46.7,36,54.8,63.5,54.1,46,67,86.3,48.7,76.8,30.7,48.3,61.4,51.5,46.2,41,38.5,79.7,56.3,85.5,76.3,84.5,69.2,56.6,44,45,83.4,62.8,42.3,44.1,40.5,55.9,89.6,53.6,74.5,90,61.4,36.3,47.8,56.9,69.7,88.7,49.3,81.9,40,46.9,51.4,50.5,50,96,62.3,48.6,49,48.3,50.9,47.7,45.3,72.9,40.7,59.5,70.5,27.2,64.2,58.6,56.3,47.2,82.4,76.1,53.9,44.2,100,78.8,57.1,55.2,54.6,52.7,48.2,41.5,70.9,30.1,33.4,46.6,43.4,44.5,77.8,42.7,53.8,57.6,44,44.9,52.5,49.7,59.6,58.4,67.2,54.6,72.1,59.6,60.2,74.4,110,90,68.8,39.8,50.8,71.3,72.5,30.8,53.6,43.1],"hoverinfo":"y","type":"box","fillcolor":"rgba(248,118,109,1)","marker":{"opacity":null,"outliercolor":"rgba(0,0,0,1)","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"},"size":5.66929133858268},"line":{"color":"rgba(51,51,51,1)","width":1.88976377952756},"name":"Mem_Score_Before","legendgroup":"Mem_Score_Before","showlegend":true,"xaxis":"x","yaxis":"y","frame":null},{"x":[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],"y":[61.2,40.7,55.1,51.2,47.1,58.1,56,74.8,45,75.9,102,63.7,40.7,84.3,32.8,56.3,44.6,72.5,65.4,49.2,44.2,73.3,52.7,46.1,54,55.5,82.9,108,46.8,70.8,79.6,56.3,50.9,50.8,65.6,44.5,88.1,49.4,96,63,102,50.8,48.1,108,64.9,66.6,74.3,87.4,114,44,55.6,69.2,63,90,88.2,67.4,120,59.7,53.4,86.4,96,77.2,60,88.5,79.7,90,75.2,73.3,90,64.2,53.6,56.7,61.4,59,48.5,50.9,44.1,61.5,81.4,41.7,47.6,45.6,59.2,90,62.9,52.1,49.4,56.8,46,35.8,65.4,65.2,59.5,43.2,70.9,79.6,52.9,78.5,27.1,47,66.4,50.2,41.3,47,41.9,88.9,56.6,83.6,74.8,44.1,65.8,53.4,38.2,46.2,67.4,54.1,28.9,41.5,33.4,60.8,89.9,48.3,75.2,80.4,57.5,40.3,49.3,58.9,71.9,96,52.7,80.6,42.2,46.9,51.4,56.8,42.2,102,66.8,50.4,40.5,44.1,41.8,37.9,41.1,74,39,61.5,65.8,37.8,57.3,52.7,56.8,49.2,83.1,73.1,49,44.5,96,84.5,53.7,51.7,54.6,53.3,55.1,46.8,67.8,30.5,57.5,52.1,47.4,47.7,72.2,48.1,45.6,60.6,40.6,48.3,60.3,41.3,56.1,63.6,64.9,59.2,74.3,44.9,55.7,82.9,87.8,82.6,77.4,44.3,30.4,74.3,70.4,33.1,53.8,42.1],"hoverinfo":"y","type":"box","fillcolor":"rgba(0,186,56,1)","marker":{"opacity":null,"outliercolor":"rgba(0,0,0,1)","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"},"size":5.66929133858268},"line":{"color":"rgba(51,51,51,1)","width":1.88976377952756},"name":"Mem_Score_After","legendgroup":"Mem_Score_After","showlegend":true,"xaxis":"x","yaxis":"y","frame":null},{"x":[3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3],"y":[-2.3,-0.9,-4.6,-0.5,0.1,-8.3,11.9,-1.5,-11.2,21.1,12,11.4,5.2,-1.3,-9.5,2.8,-3.7,8.5,-8.9,4.2,-7.9,-6.6,-3,-0.4,5.5,8.5,7.9,18,2.9,-4.1,5.1,-2.6,14.5,-8,5.7,4.3,13.9,-0.6,11.6,22.2,15,4.3,-16.3,18,4,20.2,19.1,25.6,49,15.7,13.7,19.8,19.4,18.3,33.4,25.5,39,13,21.7,20.8,38.7,4.6,6,26.9,19.9,25.9,21.9,-1.7,0,15,-0.9,7.4,-4.8,2.8,1.6,5.1,3.1,-3.6,5.2,2.1,-0.7,3.1,2.3,16,-0.4,-0.9,5.3,-2.8,-0.7,-0.2,10.6,1.7,5.4,-2.8,3.9,-6.7,4.2,1.7,-3.6,-1.3,5,-1.3,-4.9,6,3.4,9.2,0.3,-1.9,-1.5,-40.4,-3.4,-3.2,-5.8,1.2,-16,-8.7,-13.4,-2.6,-7.1,4.9,0.3,-5.3,0.7,-9.6,-3.9,4,1.5,2,2.2,7.3,3.4,-1.3,2.2,0,0,6.3,-7.8,6,4.5,1.8,-8.5,-4.2,-9.1,-9.8,-4.2,1.1,-1.7,2,-4.7,10.6,-6.9,-5.9,0.5,2,0.7,-3,-4.9,0.3,-4,5.7,-3.4,-3.5,0,0.6,6.9,5.3,-3.1,0.4,24.1,5.5,4,3.2,-5.6,5.4,-8.2,3,-3.4,3.4,7.8,-8.4,-3.5,5.2,-2.3,4.6,2.2,-14.7,-4.5,8.5,-22.2,-7.4,8.6,4.5,-20.4,3,-2.1,2.3,0.2,-1],"hoverinfo":"y","type":"box","fillcolor":"rgba(97,156,255,1)","marker":{"opacity":null,"outliercolor":"rgba(0,0,0,1)","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"},"size":5.66929133858268},"line":{"color":"rgba(51,51,51,1)","width":1.88976377952756},"name":"Diff","legendgroup":"Diff","showlegend":true,"xaxis":"x","yaxis":"y","frame":null}],"layout":{"margin":{"t":28.1765601217656,"r":7.30593607305936,"b":42.130898021309,"l":43.1050228310502},"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[0.4,3.6],"tickmode":"array","ticktext":["Mem_Score_Before","Mem_Score_After","Diff"],"tickvals":[1,2,3],"categoryorder":"array","categoryarray":["Mem_Score_Before","Mem_Score_After","Diff"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"variable","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-48.42,128.02],"tickmode":"array","ticktext":["0","50","100"],"tickvals":[0,50,100],"categoryorder":"array","categoryarray":["0","50","100"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"value","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"y":0.963910761154856},"annotations":[{"text":"variable","x":1.02,"y":1,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xref":"paper","yref":"paper","textangle":-0,"xanchor":"left","yanchor":"bottom","legendTitle":true}],"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"415c71873bcd":{"x":{},"y":{},"fill":{},"type":"box"}},"cur_data":"415c71873bcd","visdat":{"415c71873bcd":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<pre class="r"><code>rm(mem_melt) #removing temporary data frame</code></pre>
<p>To further examine the variables, the relationship between the age variables and the memory scores are plotted below. Firstly, the skewness in the score variables is fairly negligible. There is also a high correlation between the before and after scores, but perhaps that is because the difference between them is only marginal. These variables will not be used in the modelling process anyway. Apart from that, there is very little correlation seen between age and the scores, implying that perhaps age does not have much of an impact on memory recall in this specific experiment.</p>
<pre class="r"><code>mem_num &lt;- memory[, c(1, 5:7)] #temporary data frame of age and scores

ggpairs(mem_num) + theme_minimal()</code></pre>
<p><img src="/posts/Drugs-and-Memories_files/figure-html/unnamed-chunk-5-1.png" width="864" /></p>
<pre class="r"><code>rm(mem_num) #remove temporary dataframe</code></pre>
<p>A quick glimpse at the proportions of the categorical variables (Drugs, Dosage, Happy_Sad_group) can provide insights into how balanced the data is.</p>
<pre class="r"><code>prop.table(table(memory[,2])) * 100 #happy / sad memory primer</code></pre>
<pre><code>## 
##  H  S 
## 50 50</code></pre>
<pre class="r"><code>prop.table(table(memory[,3])) * 100 #dosage levels</code></pre>
<pre><code>## 
##        1        2        3 
## 33.83838 33.33333 32.82828</code></pre>
<pre class="r"><code>prop.table(table(memory[,4])) * 100 #drugs</code></pre>
<pre><code>## 
##        A        S        T 
## 33.83838 33.33333 32.82828</code></pre>
<p>It can be seen that the categorical variables are all almost equally proportional, indicating that the data in this regard is very well balanced. Moving on to the main aspect of this analysis, the interactions between the aforementioned variables needs to be observed. Acknowledging the presence of interactions amongst variables is important in statistical analysis as when they are not accounted for, the modelling results will be inaccurate since the model is unable to capture the specific relationship between the variables. To check for interactions, interaction plots are developed to identify any relationships between the three categorical variables.</p>
<pre class="r"><code>interaction.plot(memory$Drug, memory$Dosage, memory$Diff, 
                 xlab = &quot;Drug&quot;, ylab = &quot;Mean of Diff&quot;,trace.label = &quot;Dosage&quot;) #Drug-Dosage</code></pre>
<p><img src="/posts/Drugs-and-Memories_files/figure-html/unnamed-chunk-7-1.png" width="864" /></p>
<pre class="r"><code>interaction.plot(memory$Dosage, memory$Happy_Sad_group, memory$Diff,
                 xlab = &quot;Dosage&quot;, ylab = &quot;Mean of Diff&quot;,trace.label = &quot;Happy_Sad&quot;) #Dosage - Happy_Sad</code></pre>
<p><img src="/posts/Drugs-and-Memories_files/figure-html/unnamed-chunk-7-2.png" width="864" /></p>
<pre class="r"><code>interaction.plot(memory$Drug, memory$Happy_Sad_group, memory$Diff, 
                 xlab = &quot;Drug&quot;, ylab = &quot;Mean of Diff&quot;,trace.label = &quot;Happy_Sad&quot;) #Drug - Happy_Sad</code></pre>
<p><img src="/posts/Drugs-and-Memories_files/figure-html/unnamed-chunk-7-3.png" width="864" /></p>
<p>From the plots above, interaction is observed between the Drug and Dosage variables. This needs to be accounted for in the modelling process to provide accurate results. Before moving to the modelling stage, the before and after scores are removed as these are not necessary for the analysis and are already accounted for in the Diff variable.</p>
<pre class="r"><code>memory &lt;- memory[,-5:-6] #remove before and after scores as accounted for in diff</code></pre>
</div>
<div id="modelling" class="section level3">
<h3>Modelling</h3>
<p>The modelling phase begins with partitioning the data into a 70/30 train-test split. The resampling method is also defined as 10-fold repeated cross validation.</p>
<pre class="r"><code>set.seed(1234)
split &lt;- initial_split(memory, prop = 0.7)
train &lt;- training(split)
test &lt;- testing(split)

cv &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5) #10-fold repeated cross validation</code></pre>
<p>To provide a benchmark, a simple Linear Model is used, without accounting for the interaction effects, to see how the model initially performs.</p>
<pre class="r"><code>set.seed(1234)
memory_LM &lt;- train(Diff ~ .,
                   data = train,
                   method = &quot;lm&quot;,
                   trControl = cv)

memory_LM</code></pre>
<pre><code>## Linear Regression 
## 
## 139 samples
##   4 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 124, 126, 125, 125, 126, 125, ... 
## Resampling results:
## 
##   RMSE     Rsquared   MAE     
##   8.86716  0.2618596  6.757937
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>The simple linear model produces an R-squared which is not particularly impressive. To test if a more advanced model is able to do better, the Bagging method is used, implementing several decision trees and aggregating their results. It basically implements an ensemble approach. It may seem overkill for the purpose of this analysis, but this is only to compare how much a highly advanced model can improve performance.</p>
<pre class="r"><code>set.seed(1234)
memory_bagg &lt;- train(
  Diff ~ .,
  data = train,
  method = &quot;treebag&quot;,
  trControl = cv,
  nbagg = 100
)

memory_bagg</code></pre>
<pre><code>## Bagged CART 
## 
## 139 samples
##   4 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 124, 126, 125, 125, 126, 125, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   7.972879  0.3763765  5.800715</code></pre>
<p>The R-squared for the Bagged model is much better, but it is still a bit unimpressive. In the next iteration, the interaction effect will be accounted for by adding "Drug*Dosage" to the list of independent variables. This is demonstrated below by the simple LM model.</p>
<pre class="r"><code>set.seed(1234)
memory_LM_inter &lt;- train(Diff ~ Drug+Dosage+Happy_Sad_group+Drug*Dosage,
                   data = train,
                   method = &quot;lm&quot;,
                   trControl = cv)

memory_LM_inter</code></pre>
<pre><code>## Linear Regression 
## 
## 139 samples
##   3 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 124, 126, 125, 125, 126, 125, ... 
## Resampling results:
## 
##   RMSE      Rsquared  MAE     
##   7.745279  0.402875  5.682905
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>An improvement in the R-squared value as compared to the previous LM implementation is observed. The RMSE has also slightly lowered. These metrics indicate that the model is now performing much better compared to its original iteration, when accounting for interaction. The same is done for the bagged model.</p>
<pre class="r"><code>set.seed(1234)
memory_bagg_inter &lt;- train(
  Diff ~ Drug+Dosage+Happy_Sad_group+Drug*Dosage,
  data = train,
  method = &quot;treebag&quot;,
  trControl = cv,
  nbagg = 100
)

memory_bagg_inter</code></pre>
<pre><code>## Bagged CART 
## 
## 139 samples
##   3 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 124, 126, 125, 125, 126, 125, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   7.643842  0.4152214  5.564111</code></pre>
<p>An improvement from the previous Bagged iteration is seen here as well. These results clearly indicate that incorporating the interaction effect improves the explaining power of the model significantly, thereby increasing the validity of the modelling results. As an additional check, the predictive power on the test set can be checked for these models to identify any bias and variance issues.</p>
<pre class="r"><code>LM_predict_inter &lt;- predict(memory_LM_inter, test)
Bagg_predict_inter &lt;- predict(memory_bagg_inter, test)

postResample(LM_predict_inter, test$Diff)</code></pre>
<pre><code>##      RMSE  Rsquared       MAE 
## 8.7386036 0.4959643 6.4912207</code></pre>
<pre class="r"><code>postResample(Bagg_predict_inter, test$Diff)</code></pre>
<pre><code>##     RMSE Rsquared      MAE 
## 8.980167 0.462956 6.761733</code></pre>
<p>While the R-squares have improved even further, the RMSE values have marginally increased, indicating some issues with variance for the models. However, the overall objective of this analysis has been completed, which was highlighting the magnitude of the impact interaction effects have upon model performance.</p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>The above results have clearly indicated that accounting for interaction is essential in the regression process, as their influence can significantly impact how the model performs. While it may have been slightly intuitive that the drugs and their dosage are related with each other, it is still worthwhile to check for interactions if the model is performing unexpectedly poorly, or there is a suspicion about interactions based on an understanding of the data. The above results can then be further improved with more thorough modelling techniques. An example can be assessing the importance of the features. The feature importance plot according to the first LM model is shown below as an example.</p>
<pre class="r"><code>library(vip)
vip(memory_LM)</code></pre>
<p><img src="/posts/Drugs-and-Memories_files/figure-html/unnamed-chunk-15-1.png" width="864" /></p>
<p>Applying dummy encodings to the categorical variables and then selecting only the most important features can help further improve the overall model performance.</p>
</div>
]]></content>
		</item>
		
		<item>
			<title>Visualizing Happiness - World Happiness Report 2019</title>
			<link>/posts/whr-2019/</link>
			<pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
			
			<guid>/posts/whr-2019/</guid>
			<description>IntroductionThe following is a simple exercise in visualizing data. The data used is from the World Happiness Report for the year 2019, and was obtained freely from: https://www.kaggle.com/unsdsn/world-happiness?select=2019.csv. The aim is to generate some simple insights about the data using only visualizations.
Loading the data and required libraries:
#Load Librarieslibrary(tidyverse)library(reshape2)library(corrplot)#Read Data..whr19 &amp;lt;- read.csv(&amp;quot;whr-2019.csv&amp;quot;)Data processingBefore moving forward with the visuals, the data needs to be processed a little bit.</description>
			<content type="html"><![CDATA[


<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>The following is a simple exercise in visualizing data. The data used is from the World Happiness Report for the year 2019, and was obtained freely from: <a href="https://www.kaggle.com/unsdsn/world-happiness?select=2019.csv" class="uri">https://www.kaggle.com/unsdsn/world-happiness?select=2019.csv</a>. The aim is to generate some simple insights about the data using only visualizations.</p>
<p>Loading the data and required libraries:</p>
<pre class="r"><code>#Load Libraries

library(tidyverse)
library(reshape2)
library(corrplot)

#Read Data..
whr19 &lt;- read.csv(&quot;whr-2019.csv&quot;)</code></pre>
</div>
<div id="data-processing" class="section level3">
<h3>Data processing</h3>
<p>Before moving forward with the visuals, the data needs to be processed a little bit. The column names are slightly changed to look a bit more neat for the visuals, and a new feature called Region is added to group all the countries according to their respective regions. The regions are Asia, Europe, Middle East, North America, South America, Central America, The Caribbean, Oceania (Australia and New Zealand) and Africa. This is all carried out in the code chunk below.</p>
<pre class="r"><code>#change column names
colnames(whr19) &lt;- c(&quot;Rank&quot;, &quot;Country&quot;, &quot;Score&quot;,
                     &quot;GDP_per_Cap&quot;, &quot;Social_Supp&quot;, &quot;Life_Exp&quot;,
                     &quot;Freedom&quot;, &quot;Generosity&quot;, &quot;Corruption&quot;)

whr19$Country &lt;- as.character(whr19$Country)

#add new Region variable
whr19$Region &lt;- NA

whr19$Region[which(whr19$Country %in% c(&quot;Afganistan&quot;, &quot;Armenia&quot;, &quot;Azerbaijan&quot;,&quot;Bangladesh&quot;, &quot;Bhutan&quot;, &quot;Brunei Darussalam&quot;, &quot;Cambodia&quot;, &quot;China&quot;,&quot;Georgia&quot;, &quot;Hong Kong&quot;,
                                           &quot;India&quot;,&quot;Indonesia&quot;,&quot;Japan&quot;,&quot;Kazakhstan&quot;, &quot;North Korea&quot;, &quot;South Korea&quot;,&quot;Kyrgyzstan&quot;,&quot;Laos&quot;,&quot;Macao&quot;,&quot;Malaysia&quot;,&quot;Maldives&quot;,&quot;Mongolia&quot;,
                                           &quot;Myanmar&quot;,&quot;Nepal&quot;,&quot;Pakistan&quot;,&quot;Philippines&quot;,&quot;Singapore&quot;,&quot;Sri Lanka&quot;,&quot;Taiwan&quot;,&quot;Tajikistan&quot;,&quot;Thailand&quot;,&quot;Timor Leste&quot;,&quot;Turkmenistan&quot;,
                                           &quot;Uzbekistan&quot;,&quot;Vietnam&quot;))] &lt;- &quot;Asia&quot;

whr19$Region[which(whr19$Country %in% c(&quot;Norway&quot;, &quot;Denmark&quot;, &quot;Iceland&quot;, &quot;Switzerland&quot;, &quot;Finland&quot;,
                                                   &quot;Netherlands&quot;, &quot;Sweden&quot;, &quot;Austria&quot;, &quot;Ireland&quot;, &quot;Germany&quot;,
                                                   &quot;Belgium&quot;, &quot;Luxembourg&quot;, &quot;United Kingdom&quot;, &quot;Czech Republic&quot;,
                                                   &quot;Malta&quot;, &quot;France&quot;, &quot;Spain&quot;, &quot;Slovakia&quot;, &quot;Poland&quot;, &quot;Italy&quot;,
                                                   &quot;Russia&quot;, &quot;Lithuania&quot;, &quot;Latvia&quot;, &quot;Moldova&quot;, &quot;Romania&quot;,
                                                   &quot;Slovenia&quot;, &quot;Northern Cyprus&quot;, &quot;Cyprus&quot;, &quot;Estonia&quot;, &quot;Belarus&quot;,
                                                   &quot;Serbia&quot;, &quot;Hungary&quot;, &quot;Croatia&quot;, &quot;Kosovo&quot;, &quot;Montenegro&quot;,
                                                   &quot;Greece&quot;, &quot;Portugal&quot;, &quot;Bosnia and Herzegovina&quot;, &quot;Macedonia&quot;,
                                                   &quot;Bulgaria&quot;, &quot;Albania&quot;, &quot;Ukraine&quot;, &quot;Turkey&quot;, &quot;North Macedonia&quot;))] &lt;- &quot;Europe&quot;

whr19$Region[which(whr19$Country %in% c(&quot;Bahrain&quot;, &quot;Iraq&quot;, &quot;Iran&quot;, &quot;Israel&quot;,&quot;Jordan&quot;,&quot;Kuwait&quot;,&quot;Lebanon&quot;,&quot;Libya&quot;,&quot;Oman&quot;,&quot;Palestinian Territories&quot;,
                                           &quot;Qatar&quot;,&quot;Saudi Arabia&quot;,&quot;Syria&quot;,&quot;United Arab Emirates&quot;,&quot;Yemen&quot;))] &lt;- &quot;Middle East&quot;

whr19$Region[which(whr19$Country %in% c(&quot;Bermuda&quot;, &quot;Canada&quot;, &quot;Greenland&quot;, 
                                           &quot;Saint Pierre and Miquelon&quot;, &quot;United States&quot;))] &lt;- &quot;North America&quot;

whr19$Region[which(whr19$Country %in% c(&quot;Belize&quot;, &quot;Costa Rica&quot;, &quot;El Salvador&quot;, &quot;Guatemala&quot;, 
                                           &quot;Honduras&quot;,&quot;Mexico&quot;, &quot;Nicaragua&quot;, &quot;Panama&quot;))] &lt;- &quot;Central America&quot;

whr19$Region[which(whr19$Country %in% c(&quot;Anguilla&quot;, &quot;Antigua and Barbuda&quot;, &quot;Aruba&quot;, &quot;Bahamas&quot;, &quot;Barbados&quot;, &quot;Bonaire&quot;, &quot;Saint Eustatius and Saba&quot;, &quot;British Virgin Islands&quot;, &quot;Cayman Islands&quot;,
                                           &quot;Cuba&quot;, &quot;Curaçao&quot;, &quot;Dominica&quot;,&quot;Dominican Republic&quot;, &quot;Grenada&quot;, &quot;Guadeloupe&quot;, &quot;Haiti&quot;, &quot;Jamaica&quot;, &quot;Martinique&quot;, 
                                           &quot;Monserrat&quot;, &quot;Puerto Rico&quot;, &quot;Saint-Barthélemy&quot;, &quot;St. Kitts and Nevis&quot;, &quot;Saint Lucia&quot;, &quot;Saint Martin&quot;,&quot;Saint Vincent and the Grenadines&quot;,&quot;Sint Maarten&quot;, 
                                           &quot;Trinidad &amp; Tobago&quot;, &quot;Turks and Caicos Islands&quot;, &quot;Virgin Islands (US)&quot;))] &lt;- &quot;The Caribbean&quot;

whr19$Region[which(whr19$Country %in% c(&quot;Argentina&quot;,&quot;Bolivia&quot;,&quot;Brazil&quot;,&quot;Chile&quot;,&quot;Colombia&quot;,&quot;Ecuador&quot;,&quot;Falkland Islands (Malvinas)&quot;,
                                           &quot;French Guiana&quot;,&quot;Guyana&quot;,&quot;Paraguay&quot;, &quot;Peru&quot;, &quot;Suriname&quot;, &quot;Uruguay&quot;, &quot;Venezuela&quot;))] &lt;- &quot;South America&quot;

whr19$Region[which(whr19$Country %in% c(&quot;New Zealand&quot;, &quot;Australia&quot;))] &lt;- &quot;Oceania&quot;
whr19$Region[which(is.na(whr19$Region))] &lt;- &quot;Africa&quot;</code></pre>
</div>
<div id="correlation-plot-for-all-numeric-variables" class="section level3">
<h3>Correlation plot for all numeric variables</h3>
<pre class="r"><code>correlation &lt;- cor(whr19[,3:9])

corrplot(correlation, method = &quot;square&quot;)</code></pre>
<p><img src="/posts/WHR-2019_files/figure-html/unnamed-chunk-3-1.png" width="768" /></p>
<p>As the key variable here is the Score (Which determines the happiness rank), it can be seen that the Score variable shares the highest correlation with GDP per capita, Social Support and Life Expectancy. It is intersting how Generosity and Score have almost no correlation with each other (according to the data). However, in such cases it is important to remember that correlation (or lack thereof) does not imply causation.</p>
</div>
<div id="boxplot-to-analyse-the-distribution-of-different-regions" class="section level3">
<h3>Boxplot to analyse the distribution of different regions</h3>
<pre class="r"><code>box &lt;- ggplot(whr19, aes(Region, Score)) +
  geom_boxplot(aes(fill = Region)) +
  coord_flip() + 
  theme_minimal()

box #interactive</code></pre>
<p><img src="/posts/WHR-2019_files/figure-html/unnamed-chunk-4-1.png" width="864" /></p>
<p>The boxplot shows the variation in Score across the different regions. The highest variations are in the Middle East and Europe. A potential reason for this may be due to the large disparity between the very rich and poor countries in both regions. The lowest range of scores are in Africa, and the highest scoring country belongs to the European region. Oceania has the lowest variation as it is only comprised of two countries.</p>
</div>
<div id="bar-charts-for-average-regional-performance" class="section level3">
<h3>Bar charts for average regional performance</h3>
<pre class="r"><code>whr19_Region &lt;- whr19[,-1:-2] %&gt;%
  group_by(Region) %&gt;%
  summarise_all(funs(mean))

region_melt &lt;- melt(whr19_Region)

bar &lt;- ggplot(region_melt, aes(Region, value, fill = Region)) +
  geom_bar(stat = &quot;identity&quot;) +
  facet_wrap(~variable, scales = &quot;free&quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

bar</code></pre>
<p><img src="/posts/WHR-2019_files/figure-html/unnamed-chunk-5-1.png" width="864" /></p>
<p>The above grid of bar charts highlights the average value each region obtained across all the features. The Oceania region apparently achieved the highest average scores across all the features, being only surpassed slightly by North America in GDP per capita. However, Oceania only comprises of Australia and New Zealand, thus making it a bit unfair to compare it with other regional averages. Perhaps logically combining Oceania with another region might help paint a better picture of how the scores for each feature are distributed across the regions.</p>
</div>
<div id="circular-barplot-of-all-countries-happiness-score-in-order" class="section level3">
<h3>Circular barplot of all countries happiness score, in order</h3>
<p>This is just for the aesthetics. However, there are a couple of issues with this plot. Some of the country labels are a bit distorted on closer inspection. At the time of making this plot, no solution to this text distortion was found. Perhaps it is because plotting a large number of labels artificially in a circle has its difficulties and issues. The other issue is that this same information could have been easily conveyed through a simple ordered bar chart, which would have looked much more neat. Therefore, the important conclusion to draw from here is that just because it looks complex and different doesn’t always mean it is the best. On a more positive note, the circle chart does highlight the regions quite colourfully.</p>
<pre class="r"><code>circle_whr &lt;- whr19[,c(1:3,10)]
circle_whr$Score &lt;- circle_whr$Score * 10
circle_whr$Region &lt;- as.factor(circle_whr$Region)

#Groups...
circle_whr$id &lt;- seq(1, nrow(circle_whr))
circle_whr &lt;- circle_whr %&gt;% arrange(Region)

#labels...
labeled &lt;- circle_whr
n_bar &lt;- nrow(labeled)
angle &lt;- 90-360*(labeled$id-0.5)/n_bar
labeled$hjust &lt;- ifelse(angle &lt; -90, 1, 0)
labeled$angle &lt;- ifelse(angle &lt; -90, angle+180, angle)

circle &lt;- ggplot(circle_whr, aes(x = as.factor(id), y = Score, fill = Region)) +
  geom_bar(stat = &quot;identity&quot;, alpha = 0.5) +
  scale_fill_brewer(palette = &quot;Paired&quot;) +
  ylim(-100,100) +
  theme_minimal() +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        panel.grid = element_blank(),
        plot.margin = unit(rep(-1,4), &quot;cm&quot;),
        legend.position = c(0.53,0.5)) +
  coord_polar() +
  geom_text(data = labeled, aes(x = id, y = Score + 10, label = Country,
                                   hjust = hjust),
            color = &quot;black&quot;, fontface = &quot;bold&quot;, alpha = 1, size = 3,
            angle = labeled$angle, inherit.aes = FALSE)

circle</code></pre>
<p><img src="/posts/WHR-2019_files/figure-html/unnamed-chunk-6-1.png" width="864" style="display: block; margin: auto;" /></p>
</div>
]]></content>
		</item>
		
	</channel>
</rss>
